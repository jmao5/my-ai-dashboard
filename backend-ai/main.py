from fastapi import FastAPI, Depends, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sqlalchemy.orm import Session
import database
import os
import google.generativeai as genai

# 1. DB ì´ˆê¸°í™”
database.Base.metadata.create_all(bind=database.engine)

# 2. Gemini ì„¤ì • (ì¤‘ë³µ ì½”ë“œ ì œê±° ë° ìµœì‹  ëª¨ë¸ ì„¤ì •)
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")
model = None

if not GOOGLE_API_KEY:
    print("âš ï¸ ê²½ê³ : GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
else:
    genai.configure(api_key=GOOGLE_API_KEY)

    # ëª¨ë¸ ê³ ì • (ê°€ì¥ ë¹ ë¥´ê³  ìµœì‹ ì¸ flash ëª¨ë¸ ì¶”ì²œ)
    target_model = 'gemini-2.5-flash'

    print(f"ğŸš€ AI ëª¨ë¸ '{target_model}' ë¡œë“œ ì¤‘...")
    try:
        model = genai.GenerativeModel(target_model)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# ë¡œê·¸ ë¶„ì„ ìš”ì²­ìš© ë°ì´í„° êµ¬ì¡°
class AnalysisRequest(BaseModel):
    log_text: str

def get_db():
    db = database.SessionLocal()
    try:
        yield db
    finally:
        db.close()

@app.get("/")
def read_root():
    return {"message": "Gemini AI Server is Running!"}

@app.get("/api/ai-status")
def get_ai_status():
    status = "Online" if model else "Offline"
    return {
        "status": status,
        "model": model.model_name if model else "None",
        "message": "AIê°€ ì´ì œ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤! ğŸ§ "
    }

@app.get("/api/chat/history")
def get_chat_history(db: Session = Depends(get_db)):
    # ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜¤ë˜, ë‹¤ì‹œ ì‹œê°„ìˆœ(ê³¼ê±°->í˜„ì¬)ìœ¼ë¡œ ì •ë ¬í•´ì•¼ ì±„íŒ…ì°½ì— ì œëŒ€ë¡œ ë³´ì„
    history = db.query(database.ChatHistory).order_by(database.ChatHistory.id.desc()).limit(50).all()
    # íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ë’¤ì§‘ê¸° ([::-1]) -> ê³¼ê±°ë¶€í„° í˜„ì¬ ìˆœì„œë¡œ
    return [{"role": h.role, "text": h.message} for h in history[::-1]]

@app.post("/api/upload")
async def upload_document(file: UploadFile = File(...), db: Session = Depends(get_db)):
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸° (í…ìŠ¤íŠ¸ íŒŒì¼ì´ë¼ê³  ê°€ì •)
        content = await file.read()
        text_content = content.decode("utf-8")

        # DBì— ì €ì¥
        db_doc = database.Document(filename=file.filename, content=text_content)
        db.add(db_doc)
        db.commit()

        return {"message": f"íŒŒì¼ '{file.filename}' í•™ìŠµ ì™„ë£Œ!", "preview": text_content[:100] + "..."}
    except Exception as e:
        print(f"Upload Error: {e}")
        raise HTTPException(status_code=400, detail="íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸(.txt, .md, .conf) íŒŒì¼ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# ğŸ‘‡ [ìˆ˜ì •] ì±„íŒ… API (ì§€ì‹ ì°¸ì¡° ê¸°ëŠ¥ ì¶”ê°€)
@app.post("/api/chat")
async def chat_with_ai(request: ChatRequest, db: Session = Depends(get_db)):
    user_msg = request.message

    # 1. ìœ ì € ë©”ì‹œì§€ DB ì €ì¥
    db_user_msg = database.ChatHistory(role="user", message=user_msg)
    db.add(db_user_msg)
    db.commit()

    ai_response = ""

    try:
        if not GOOGLE_API_KEY or not model:
            ai_response = "ì‹œìŠ¤í…œ ì˜¤ë¥˜: AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
        else:
            # === ğŸ§  RAG í•µì‹¬ ë¡œì§ ===
            # ê°€ì¥ ìµœê·¼ì— ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ê°„ì´ RAG)
            # ë‚˜ì¤‘ì—ëŠ” Vector DBë¥¼ ì¨ì„œ ê´€ë ¨ëœ ê²ƒë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            latest_doc = db.query(database.Document).order_by(database.Document.id.desc()).first()

            context_prompt = ""
            if latest_doc:
                context_prompt = f"""
                [Reference Document: {latest_doc.filename}]
                {latest_doc.content}
                -----------------------------------
                ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì•„ë˜ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
                ì‚¬ìš©ì ì§ˆë¬¸: {user_msg}
                """
            else:
                context_prompt = user_msg # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì§ˆë¬¸ë§Œ
            # =========================

            # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            recent_history = db.query(database.ChatHistory) \
                .order_by(database.ChatHistory.id.desc()) \
                .limit(10).all()

            gemini_history = []
            for msg in reversed(recent_history):
                role = "user" if msg.role == "user" else "model"
                if msg.message == user_msg and msg.role == 'user': continue
                gemini_history.append({"role": role, "parts": [msg.message]})

            chat_session = model.start_chat(history=gemini_history)

            # ì§ˆë¬¸ ì „ì†¡ (ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ ì „ì†¡)
            response = chat_session.send_message(context_prompt)
            ai_response = response.text

    except Exception as e:
        ai_response = f"Error: {str(e)}"
        print(f"Gemini Error: {e}")

    # AI ë‹µë³€ ì €ì¥
    db_ai_msg = database.ChatHistory(role="bot", message=ai_response)
    db.add(db_ai_msg)
    db.commit()

    return {"reply": ai_response}

# ë¡œê·¸ ë¶„ì„ ì „ìš© API
@app.post("/api/analyze/log")
async def analyze_log(request: AnalysisRequest):
    log_content = request.log_text

    # ë¡œê·¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¦…ë‹ˆë‹¤ (í† í° ì œí•œ ë° ë¹„ìš© ì ˆì•½)
    if len(log_content) > 5000:
        log_content = log_content[-5000:] # ë’¤ì—ì„œë¶€í„° 5000ì

    # ì‹œìŠ¤í…œ ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜ ë¶€ì—¬)
    prompt = f"""
    You are a Senior Linux System Administrator and DevOps Engineer.
    Please analyze the following server logs and provide a report in Korean(í•œêµ­ì–´).
    
    [Logs]
    {log_content}
    
    [Instructions]
    1. Summarize the key events.
    2. Identify any Errors or Warnings.
    3. Suggest specific solutions or commands to fix the issues.
    4. Use Markdown format (bold, code blocks).
    """

    try:
        if not model:
            return {"reply": "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        response = model.generate_content(prompt)
        return {"reply": response.text}

    except Exception as e:
        print(f"Analysis Error: {e}")
        return {"reply": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)